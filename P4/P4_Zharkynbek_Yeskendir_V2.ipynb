{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc61e56-5c17-4571-9145-807b31624cf6",
   "metadata": {},
   "source": [
    "# **Project 4: PointCloud-LLM (PC-LLM)**\n",
    "\n",
    "In this project, you will learn how to adapt a large language model (LLM) to handle **non-text inputs**, specifically **3D point clouds**. The objective is to enable the LLM to generate **human-like descriptions** for input 3D point clouds.\n",
    "\n",
    "The project is similar to [PointLLM](https://arxiv.org/abs/2308.16911) with totally different LLM and heavily optimized code.\n",
    "\n",
    "The dataset consists of approximately **500K descriptions** from the [Cap3D](https://arxiv.org/abs/2306.07279) dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this project, you will understand how to:\n",
    "\n",
    "- Project a non-text modality (3D point clouds) into the LLM embedding space.  \n",
    "- Introduce and train **special tokens** within the LLM.  \n",
    "- Construct **multimodal prompts** using the latest **Hugging Face** interface.  \n",
    "- Fine-tune the model with **LoRA (Low-Rank Adaptation)** using the `peft` library.  \n",
    "- Save and load LoRA fine-tuned models.  \n",
    "- Explore different **token generation strategies**, including greedy decoding and top-p sampling.\n",
    "\n",
    "\n",
    "**Requirement** âš ï¸: Please implement all the TODOS!\n",
    "---\n",
    "\n",
    "## **Model Architecture**\n",
    "\n",
    "The overall architecture is illustrated below:  \n",
    "![img](PC_LLM.png)\n",
    "\n",
    "We will use a **LLaMA 3.2 (1B)** model as the language backbone, combined with a **pretrained PointBERT** encoder for processing point cloud inputs.\n",
    "\n",
    "For simplicity, the **point cloud feature** will be represented as a **single token**, obtained by **max pooling** the PointBERT 512 embedding vectors and concatenating the max pooled vector with the CLS vector.\n",
    "\n",
    "---\n",
    "\n",
    "## **Training Setup**\n",
    "\n",
    "- **Hardware requirements:**  \n",
    "  - 1 Ã— GPU with **24 GB VRAM** (e.g., RTX 3090)  \n",
    "  - **10 GB** CPU RAM  \n",
    "\n",
    "- **Training time:** ~**2 hours 35 minutes**  \n",
    "- **Hyperparameter tuning:** Not required if the implementation is correct â€” a single training run should suffice.\n",
    "\n",
    "---\n",
    "\n",
    "## **Resources**\n",
    "\n",
    "## The following resources are required and can be downloaded [here](https://drive.google.com/drive/folders/1XCJhVgT_YQ5ocwFEBGIudSVFxlV0UX5T?usp=sharing) ðŸŒ:  \n",
    "- LLaMA 3.2 (1B) model  \n",
    "- Pretrained PointBERT encoder  \n",
    "- Cap3D dataset\n",
    "- Shapes colored Pointclouds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc374d-917c-41d8-a62a-c6a34106de4f",
   "metadata": {},
   "source": [
    "# Preparing the environment\n",
    "The code is tested on cuda 11.8, pytorch 2.5.1 (please make sure you are using the exact version), and Python 3.10.19\n",
    "We assume at least cuda 11.7 is installed. \n",
    "\n",
    "```bash\n",
    "conda create -n llama32 python=3.10 -y\n",
    "conda activate llama32\n",
    "\n",
    "pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install transformers==4.46.2 accelerate==0.33.0 safetensors==0.4.5 sentencepiece==0.2.0\\\n",
    "pip install bitsandbytes==0.43.2\n",
    "pip install huggingface_hub==0.24.6\n",
    "pip install omegaconf timm matplotlib termcolor tqdm peft plotly gradio\n",
    "conda install jupyter\n",
    "```\n",
    "\n",
    "# Extracting the point cloud\n",
    "\n",
    "```\n",
    "cat Objaverse_660K_8192_npy_split_a* > Objaverse_660K_8192_npy.tar.gz\n",
    "tar -xvf Objaverse_660K_8192_npy.tar.gz\n",
    "```\n",
    "\n",
    "# Other files/folders\n",
    "- `PointLLM_brief_description_val_3000_GT.json`: will be the test set\n",
    "- `pointllm_500k_dataset.json`: will be the training set\n",
    "- `point_bert_v1.2.pt` is the pretrained PointBert\n",
    "- `Llama-3.2-1B-Instruct` contains LLama3.2 1B model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1071d070-4d85-42dc-b8b0-b9ac8aab0354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "GPU 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.device_count()) \n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0e37789b-33db-45e5-aae8-49fe04703567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Please use your appropriate gpu id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9780e11d-554b-4e9f-9aa2-9ff6f91e597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pointbert.point_encoder import *\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d4cf458f-01ad-4b8d-bdf2-009bb90888ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"Please make sure the used pytorch version runs on the gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4abc5191-6d7d-4ba5-8164-bd74cc3d2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Config\n",
    "# ------------------------------------------------------------\n",
    "MODEL_ID = \"/ibex/user/zharkyy/P4_lab/Llama-3.2-1B-Instruct\" \n",
    "point_bert_checkpoint_path = \"/ibex/user/zharkyy/P4_lab/point_bert_v1.2.pt\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "# Used for optimization pruporses to make the code fit inside a single gpu of 24GB\n",
    "DTYPE = torch.bfloat16 \n",
    "\n",
    "THREE_D_DIM = 768 # The 3D feature dimension\n",
    "BATCH_SIZE = 32 # The batch size\n",
    "EPOCHS = 1 # We will train for a single epoch\n",
    "MAX_LEN = 384 # The maximum allowed context length of the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "84e40dd9-6b6c-48c2-a261-864a38cef073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab initial size: 128256\n",
      "Tokenizer vocab size after adding 3 special tokens: 128259\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Tokenizer, Model & Adding the special tokens\n",
    "# ------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer vocab initial size: {len(tokenizer)}\")\n",
    "newly_added_tokens_count = 0\n",
    "POINT_CLOUD_TOKEN = \"<POINT_CLOUD>\"\n",
    "POINT_START_TOKEN = \"<POINT_START>\"\n",
    "POINT_END_TOKEN = \"<POINT_END>\"\n",
    "\n",
    "# TODO write your code here\n",
    "new_tokens = {\n",
    "    'additional_special_tokens': [\n",
    "        POINT_CLOUD_TOKEN,\n",
    "        POINT_START_TOKEN,\n",
    "        POINT_END_TOKEN\n",
    "    ]\n",
    "}\n",
    "newly_added_tokens_count = tokenizer.add_special_tokens(new_tokens)\n",
    "\n",
    "# Sanity check\n",
    "assert newly_added_tokens_count == 3\n",
    "print(f\"Tokenizer vocab size after adding 3 special tokens: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f91221e-1b4d-4bda-879c-e3721a17ea89",
   "metadata": {},
   "source": [
    "## Constructing the LLM Training Prompt\n",
    "\n",
    "This section explains how to build the input prompt for the LLM (LLaMA-based) during training.\n",
    "\n",
    "First please have a look [here](https://huggingface.co/docs/transformers/en/chat_templating).\n",
    "\n",
    "```python\n",
    "model_test_messages = [\n",
    "     {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes 3D shapes.\"},\n",
    "     {\"role\": \"user\", \"content\": f\"Please provide a caption for the input shape: {POINT_START_TOKEN}{POINT_CLOUD_TOKEN}{POINT_END_TOKEN}\"},\n",
    "]\n",
    "\n",
    "model_train_messages = model_test_messages + [\n",
    "    {\"role\": \"assistant\", \"content\": \"<<<CAPTION>>>\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### Key Guidelines\n",
    "\n",
    "1. **Model Objective**\n",
    "   - The LLM should **predict only the shape caption tokens**.\n",
    "   - Only the caption tokens should be marked as ground-truth (`label`) tokens.\n",
    "   - All **non-caption tokens**, including padding, should be **masked with `-100`**.\n",
    "\n",
    "2. **Prompt Construction**\n",
    "   - Use `self.tok.apply_chat_template()` and `self.tok()` to handle tokenization and chat-style formatting.\n",
    "   - Enable **truncation** and set the **maximum context length** to `MAX_LEN`.\n",
    "\n",
    "3. **Two-Stage Tokenization Strategy**\n",
    "   - **First call:**  \n",
    "     - Generate tokens for the input text *without* the shape caption.  \n",
    "     - Use this to determine the number of input (non-labels) tokens.  \n",
    "     - Set `add_generation_prompt=True` to avoid predicting `\"ASSISTANT:\"` tokens part.\n",
    "   - **Second call:**  \n",
    "     - Generate tokens for the **full conversation** (including the shape caption).  \n",
    "     - Set `add_generation_prompt=False` since the assistant message (caption) is already part of the input.\n",
    "\n",
    "4. **Point Cloud Feature Integration**\n",
    "   - During the training/inference, the embedding of the `POINT_CLOUD_TOKEN` (at index `pc_token_pos`) will be replaced with the **shape point cloud feature** extracted from **PointBERT**. `POINT_CLOUD_TOKEN` is just used as a placeholder.\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip:** Following this process ensures that the LLaMA model only learns to generate descriptive captions for point clouds, without being confused by instruction or role tokens. Please have a look at the construct_prompt for the test part for hints. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1016432c-2e6e-4007-8ad5-fdee8dd3f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Dataset class and visualize the point cloud\n",
    "# ------------------------------------------------------------\n",
    "class ShapeCaptionDataset(Dataset):\n",
    "    def __init__(self, pointclouds_dir, captions_file_path, tokenizer, pointcloud_token, max_len=MAX_LEN, mode='train', debug=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load the point clouds\n",
    "        self.pointclouds_path = pointclouds_dir\n",
    "        self.tok = tokenizer\n",
    "        self.mode = mode\n",
    "        self.max_len = max_len\n",
    "        self.pointcloud_token_id = tokenizer.convert_tokens_to_ids(pointcloud_token)\n",
    "\n",
    "        # Load the annotations\n",
    "        self.captions = []\n",
    "        with open(captions_file_path) as fin:\n",
    "            captions = json.load(fin)\n",
    "        \n",
    "        filter_ids = ['6760e543e1d645d5aaacd3803bcae524', 'b91c0711149d460a8004f9c06d3b7f38'] # Bad examples without color needs to be excluded\n",
    "        \n",
    "        for data in captions:\n",
    "            object_id = data['object_id']\n",
    "            if object_id in filter_ids:\n",
    "                continue\n",
    "            self.captions.append(data)\n",
    "\n",
    "        if debug is not None:\n",
    "            self.captions = self.captions[:debug]\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.captions)\n",
    "    \n",
    "    def pc_norm(self, pc):\n",
    "        \"\"\" \n",
    "        Center the point cloud around the origin point scaled to fit inside a unit sphere\n",
    "        pc: Nx6 (XYZ RGB)\n",
    "        Return the normalized pc Nx6 \n",
    "        \"\"\"\n",
    "        # TODO write your code here\n",
    "        # pc = None\n",
    "        xyz = pc[:, :3]\n",
    "        \n",
    "        centroid = np.mean(xyz, axis=0)\n",
    "        xyz = xyz - centroid\n",
    "        \n",
    "        m = np.max(np.sqrt(np.sum(xyz**2, axis=1)))\n",
    "        xyz = xyz / m\n",
    "        \n",
    "        pc[:, :3] = xyz\n",
    "        \n",
    "        return pc\n",
    "\n",
    "    def construct_prompt(self, caption):\n",
    "        if self.mode == 'train':\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes 3D shapes.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Please provide a caption for the input shape: {POINT_START_TOKEN}{POINT_CLOUD_TOKEN}{POINT_END_TOKEN}\"},\n",
    "                {\"role\": \"assistant\", \"content\": caption} # Add the shape caption\n",
    "            ]\n",
    "        elif self.mode == 'test':\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes 3D shapes.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Please provide a caption for the input shape: {POINT_START_TOKEN}{POINT_CLOUD_TOKEN}{POINT_END_TOKEN}\"},\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            # TODO write your code here\n",
    "            messages_without_caption = messages[:-1]  \n",
    "            prompt_without_caption = self.tok.apply_chat_template(\n",
    "                messages_without_caption, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            enc_without_caption = self.tok(\n",
    "                prompt_without_caption, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=self.max_len\n",
    "            )\n",
    "            num_input_tokens = enc_without_caption['input_ids'].shape[1]\n",
    "            \n",
    "            prompt_with_caption = self.tok.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            enc_with_caption = self.tok(\n",
    "                prompt_with_caption, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=self.max_len\n",
    "            )\n",
    "            \n",
    "            input_ids = enc_with_caption['input_ids'].squeeze(0)\n",
    "            attention_mask = enc_with_caption['attention_mask'].squeeze(0)\n",
    "            \n",
    "            labels = input_ids.clone()\n",
    "            labels[:num_input_tokens] = -100  # Mask input tokens\n",
    "            labels[input_ids == self.tok.pad_token_id] = -100  # Mask padding tokens\n",
    "            \n",
    "            pc_token_pos = (input_ids == self.pointcloud_token_id).nonzero(as_tuple=True)[0]\n",
    "            pc_token_pos = pc_token_pos.item() if len(pc_token_pos) > 0 else -1\n",
    "            \n",
    "            # pc_token_pos is the index of the POINT_CLOUD_TOKEN inside the prompt, what do you think should it be the same for all descriptions?\n",
    "            # No, it can vary because different prompts may have different lengths. However, in our case with a fixed prompt, the position should be the same for all samples.\n",
    "            return input_ids, attention_mask, labels, pc_token_pos\n",
    "            \n",
    "        elif self.mode == 'test':\n",
    "            # Note that there should be no labels here as this is the inference mode\n",
    "            prompt = self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "            \n",
    "            # TODO write your code here\n",
    "            input_ids = enc['input_ids'].squeeze(0)\n",
    "            attention_mask = enc['attention_mask'].squeeze(0)\n",
    "            \n",
    "            # Find the position of POINT_CLOUD_TOKEN\n",
    "            pc_token_pos = (input_ids == self.pointcloud_token_id).nonzero(as_tuple=True)[0]\n",
    "            pc_token_pos = pc_token_pos.item() if len(pc_token_pos) > 0 else -1\n",
    "            \n",
    "            labels = [] # Keep it as is. we do not need to provide labels during the inference\n",
    "            \n",
    "            return input_ids, attention_mask, labels, pc_token_pos\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        data = self.captions[idx]\n",
    "        \n",
    "        # Get the object id\n",
    "        object_id = data['object_id']\n",
    "        \n",
    "        # Get the caption\n",
    "        caption = data['conversations'][-1]['value']\n",
    "        \n",
    "        # Get the colored point cloud\n",
    "        filename = f\"{object_id}_8192.npy\"    \n",
    "        point_cloud = np.load(os.path.join(self.pointclouds_path, filename))\n",
    "        point_cloud = self.pc_norm(point_cloud) # * need to norm since point encoder is norm\n",
    "\n",
    "        input_ids, attention_mask, labels, pc_token_pos = self.construct_prompt(caption)\n",
    "                \n",
    "        return dict(pointcloud=point_cloud, \n",
    "                    caption=caption, \n",
    "                    object_id=object_id, \n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels, \n",
    "                    pc_token_pos=pc_token_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b5941277-d7dc-434c-8c5d-d16563bfe1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Batch:\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    obj_positions: torch.Tensor\n",
    "    pointclouds: torch.Tensor\n",
    "    gt_captions: list\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, items: List[Dict[str, Any]]) -> Batch:\n",
    "        input_ids, attention_mask, labels, obj_positions, gt_captions = [], [], [], [], []\n",
    "        pointclouds = [] \n",
    "\n",
    "        for example_data in items:\n",
    "            input_ids.append(example_data['input_ids'])\n",
    "            attention_mask.append(example_data['attention_mask'])\n",
    "            labels.append(example_data['labels'])\n",
    "            obj_positions.append(example_data['pc_token_pos'])\n",
    "            gt_captions.append(example_data[\"caption\"])\n",
    "            pointclouds.append(torch.from_numpy(example_data['pointcloud']).float())\n",
    "\n",
    "\n",
    "\n",
    "        # Stack into batch with the necessary padding if needed \n",
    "        # TODO write your code here, have a look at the nn.utils.rnn.pad_sequence fucntion\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "        \n",
    "        # labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        # pointclouds = torch.stack(pointclouds, dim=0)\n",
    "        if labels and isinstance(labels[0], torch.Tensor):\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        else:\n",
    "            labels = []  # Keep as empty list for test mode\n",
    "        \n",
    "        pointclouds = torch.stack(pointclouds, dim=0)\n",
    "\n",
    "        return Batch(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            obj_positions=torch.tensor(obj_positions, dtype=torch.long),\n",
    "            pointclouds=pointclouds,\n",
    "            gt_captions=gt_captions\n",
    "        )\n",
    "        \n",
    "collator = Collator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cf6726e9-3521-4b5d-9a0e-127a4ead7467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The len of the training dataset is 499998\n",
      "The len of the test dataset is 3000\n"
     ]
    }
   ],
   "source": [
    "# Create the training dataset\n",
    "train_ds = ShapeCaptionDataset(\n",
    "    pointclouds_dir=\"/ibex/user/zharkyy/P4_lab/8192_npy\",\n",
    "    captions_file_path=\"/ibex/user/zharkyy/P4_lab/pointllm_500k_dataset.json\",\n",
    "    tokenizer=tokenizer, \n",
    "    pointcloud_token=POINT_CLOUD_TOKEN,\n",
    "    mode='train',\n",
    "    debug=None # You can can write a small number (like 500 training example) for debugging\n",
    ")\n",
    "\n",
    "print(f\"The len of the training dataset is {len(train_ds)}\")\n",
    "\n",
    "test_ds = ShapeCaptionDataset(\n",
    "    pointclouds_dir=\"/ibex/user/zharkyy/P4_lab/8192_npy\",\n",
    "    captions_file_path=\"/ibex/user/zharkyy/P4_lab/PointLLM_brief_description_val_3000_GT.json\",\n",
    "    tokenizer=tokenizer, \n",
    "    pointcloud_token=POINT_CLOUD_TOKEN,\n",
    "    mode='test',\n",
    ")\n",
    "\n",
    "print(f\"The len of the test dataset is {len(test_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator, drop_last=True) \n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collator, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "34474516-4194-452e-ba40-bd900b59a53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model embedding size: torch.Size([128259, 2048])\n",
      "Trainable parameters in embeddings: 6144\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # torch_dtype=\"auto\",\n",
    "    dtype = \"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Resize the input and output embedding layers to accommodate the three newly added tokens\n",
    "# Please have a look here: https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings\n",
    "# to see how the tokens are initialized? please make sure you are reading the most recent docuementation.\n",
    "# Please read this https://www.cs.columbia.edu/~johnhew/vocab-expansion.html\n",
    "# TODO write your code here\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# Freeze all LLM params\n",
    "# Use simple loop; i.e. for p in model.parameters()\n",
    "# TODO write your code here\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.get_input_embeddings().weight[-3:].requires_grad = True\n",
    "print(f\"Model embedding size: {model.get_input_embeddings().weight.shape}\")\n",
    "print(f\"Trainable parameters in embeddings: {model.get_input_embeddings().weight[-3:].numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd38c8-689d-45e6-8265-7ef3aeb95aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Construct the LoRA Config \n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from peft import get_peft_model, PeftModel\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                           # Rank of the LoRA matrices\n",
    "    lora_alpha=32,                 # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,             # Dropout on LoRA layers\n",
    "    bias=\"none\",\n",
    "    task_type= TaskType.CAUSAL_LM # TODO+\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c11fed3c-f521-4e53-9cc2-dec99ab26c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** LoRA trainable parameters *****\n",
      "trainable params: 851,968 || all params: 1,236,672,512 || trainable%: 0.0689\n",
      "***** The full trainable parameters *****\n",
      "trainable params: 851,968 || all params: 1,236,672,512 || trainable%: 0.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ibex/user/zharkyy/conda-environments/llama32/lib/python3.10/site-packages/peft/mapping_func.py:72: UserWarning:\n",
      "\n",
      "You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the peft model \n",
    "model = get_peft_model(model, lora_config)\n",
    "model.train()\n",
    "print(\"***** LoRA trainable parameters *****\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# make sure that also the input embedding layers are trainable, useful function model.get_input_embeddings()\n",
    "\n",
    "# What is the output embedding layer? \n",
    "# The output embedding layer shares weights with input embeddings in LLaMA\n",
    "\n",
    "# Do we need to modify the weights of the output embedding layer?\n",
    "# Since we've already unfrozen the new token embeddings in input_embeddings, output embedding layer will also have gradients for those tokens\n",
    "# We don't need to separately modify lm_head as it typically shares weights with input embeddings\n",
    "\n",
    "# TODO write your code here\n",
    "model.get_input_embeddings().weight[-3:].requires_grad = True\n",
    "\n",
    "\n",
    "print(\"***** The full trainable parameters *****\") # should be higher \n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9e16249a-d21d-4283-875e-4e56a0822557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved model to GPU\n"
     ]
    }
   ],
   "source": [
    "model.to(DEVICE)\n",
    "print(\"Moved model to GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "97d547a6-c21a-4512-8dff-c483267fbef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM hidden dimension: 2048\n"
     ]
    }
   ],
   "source": [
    "# TODO write your code here\n",
    "llm_hidden_dim = model.get_input_embeddings().weight.shape[1]\n",
    "print(f\"LLM hidden dimension: {llm_hidden_dim}\")\n",
    "################################################################\n",
    "#####################    3D Projector   ########################\n",
    "################################################################\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, width: int = 1024):\n",
    "        super().__init__()\n",
    "\n",
    "        # 2 linear layers with hidden dim of 1024 and use one GELU between the two linear layers\n",
    "        # TODO write your code here\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, width),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(width, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "projector = Projector(THREE_D_DIM, llm_hidden_dim).to(DEVICE).to(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6dea8a18-02d0-4636-9498-8ed8bfb78137",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "######################     PointBert   ######################### \n",
    "################################################################\n",
    "class PointEncoder(nn.Module):\n",
    "    def __init__(self, config_path: str=\"pointbert_config.yaml\", flant5_hidden_dim: int = 2048):\n",
    "        super(PointEncoder, self).__init__()\n",
    "        \n",
    "        # Read the config\n",
    "        point_bert_config = OmegaConf.load(config_path)\n",
    "        point_bert_config.model.point_dims = 6\n",
    "        use_max_pool = True\n",
    "        \n",
    "        self.point_backbone = PointTransformer(point_bert_config.model, use_max_pool=use_max_pool)\n",
    "        self.point_backbone_config = {\n",
    "            \"point_cloud_dim\": point_bert_config.model.point_dims,\n",
    "            \"backbone_output_dim\": point_bert_config.model.trans_dim if not use_max_pool else point_bert_config.model.trans_dim * 2,\n",
    "            \"project_output_dim\": flant5_hidden_dim,\n",
    "            \"point_token_len\": point_bert_config.model.num_group + 1 if not use_max_pool else 1, # * number of output features, with cls token\n",
    "            \"projection_hidden_layer\": point_bert_config.model.get('projection_hidden_layer', 0),\n",
    "            \"use_max_pool\": use_max_pool\n",
    "        }\n",
    "        \n",
    "        if point_bert_config.model.get('projection_hidden_layer', 0) > 0:\n",
    "            self.point_backbone_config[\"projection_hidden_dim\"] = point_bert_config.model.projection_hidden_dim \n",
    "        \n",
    "    def load_point_backbone_checkpoint(self, checkpoint_path=None):\n",
    "        self.point_backbone.load_checkpoint(checkpoint_path)\n",
    "        \n",
    "    def forward(self, point_clouds):\n",
    "        point_features = self.point_backbone(point_clouds)\n",
    "        return point_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bf521d8f-6fc5-410e-b519-9a86ff520df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ibex/user/zharkyy/P4_lab/pointbert/point_encoder.py:145: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "2025-12-07 15:03:44,069 - Transformer - INFO - PointBERT's weights are successfully loaded from /ibex/user/zharkyy/P4_lab/point_bert_v1.2.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and loaded the point encoder\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained checkpoint\n",
    "point_encoder = PointEncoder().to(DEVICE).to(DTYPE)\n",
    "point_encoder.load_point_backbone_checkpoint(point_bert_checkpoint_path)\n",
    "point_encoder.eval() # do not forget to convert it to evaluation mode\n",
    "\n",
    "print(\"Created and loaded the point encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "db77f801-e92e-4625-906a-0b8164eecedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1005666/518217905.py:11: FutureWarning:\n",
      "\n",
      "`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Training setup\n",
    "# ------------------------------------------------------------\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.parameters(), \"lr\": 0.0002, \"weight_decay\": 0.00},   # model\n",
    "    {\"params\": projector.parameters(), \"lr\": 0.001, \"weight_decay\": 0.0} # projector\n",
    "])\n",
    "\n",
    "num_steps = EPOCHS * math.ceil(len(train_loader))\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max(1, num_steps // 20), num_training_steps=num_steps)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fd7f94c1-7ed2-4452-a576-6d6a8e7068fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78124c7dd14a4881bc306fe2137a23ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trainig:   0%|          | 0/15624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Step 0000 | Loss 3.7458\n",
      "Epoch 0 | Step 0250 | Loss 1.6609\n",
      "Epoch 0 | Step 0500 | Loss 1.6463\n",
      "Epoch 0 | Step 0750 | Loss 1.7157\n",
      "Epoch 0 | Step 1000 | Loss 1.6277\n",
      "Epoch 0 | Step 1250 | Loss 1.6395\n",
      "Epoch 0 | Step 1500 | Loss 1.5725\n",
      "Epoch 0 | Step 1750 | Loss 1.4899\n",
      "Epoch 0 | Step 2000 | Loss 1.4850\n",
      "Epoch 0 | Step 2250 | Loss 1.6715\n",
      "Epoch 0 | Step 2500 | Loss 1.4016\n",
      "Epoch 0 | Step 2750 | Loss 1.2402\n",
      "Epoch 0 | Step 3000 | Loss 1.2801\n",
      "Epoch 0 | Step 3250 | Loss 1.4786\n",
      "Epoch 0 | Step 3500 | Loss 1.2130\n",
      "Epoch 0 | Step 3750 | Loss 1.3339\n",
      "Epoch 0 | Step 4000 | Loss 1.2528\n",
      "Epoch 0 | Step 4250 | Loss 1.3375\n",
      "Epoch 0 | Step 4500 | Loss 1.3838\n",
      "Epoch 0 | Step 4750 | Loss 1.5100\n",
      "Epoch 0 | Step 5000 | Loss 1.2210\n",
      "Epoch 0 | Step 5250 | Loss 1.2865\n",
      "Epoch 0 | Step 5500 | Loss 1.2775\n",
      "Epoch 0 | Step 5750 | Loss 1.2869\n",
      "Epoch 0 | Step 6000 | Loss 1.3548\n",
      "Epoch 0 | Step 6250 | Loss 1.3973\n",
      "Epoch 0 | Step 6500 | Loss 1.2057\n",
      "Epoch 0 | Step 6750 | Loss 1.4344\n",
      "Epoch 0 | Step 7000 | Loss 1.1937\n",
      "Epoch 0 | Step 7250 | Loss 1.3882\n",
      "Epoch 0 | Step 7500 | Loss 1.4499\n",
      "Epoch 0 | Step 7750 | Loss 1.1486\n",
      "Epoch 0 | Step 8000 | Loss 1.4591\n",
      "Epoch 0 | Step 8250 | Loss 1.1781\n",
      "Epoch 0 | Step 8500 | Loss 1.1981\n",
      "Epoch 0 | Step 8750 | Loss 1.1369\n",
      "Epoch 0 | Step 9000 | Loss 1.3524\n",
      "Epoch 0 | Step 9250 | Loss 1.3773\n",
      "Epoch 0 | Step 9500 | Loss 1.3762\n",
      "Epoch 0 | Step 9750 | Loss 1.3015\n",
      "Epoch 0 | Step 10000 | Loss 1.3304\n",
      "Epoch 0 | Step 10250 | Loss 1.1826\n",
      "Epoch 0 | Step 10500 | Loss 1.2396\n",
      "Epoch 0 | Step 10750 | Loss 1.1801\n",
      "Epoch 0 | Step 11000 | Loss 1.3689\n",
      "Epoch 0 | Step 11250 | Loss 1.3430\n",
      "Epoch 0 | Step 11500 | Loss 1.0800\n",
      "Epoch 0 | Step 11750 | Loss 1.3543\n",
      "Epoch 0 | Step 12000 | Loss 1.1837\n",
      "Epoch 0 | Step 12250 | Loss 1.4079\n",
      "Epoch 0 | Step 12500 | Loss 1.4791\n",
      "Epoch 0 | Step 12750 | Loss 1.1505\n",
      "Epoch 0 | Step 13000 | Loss 1.2995\n",
      "Epoch 0 | Step 13250 | Loss 1.5496\n",
      "Epoch 0 | Step 13500 | Loss 1.2699\n",
      "Epoch 0 | Step 13750 | Loss 1.2872\n",
      "Epoch 0 | Step 14000 | Loss 1.2454\n",
      "Epoch 0 | Step 14250 | Loss 1.1671\n",
      "Epoch 0 | Step 14500 | Loss 1.1456\n",
      "Epoch 0 | Step 14750 | Loss 1.2657\n",
      "Epoch 0 | Step 15000 | Loss 1.2861\n",
      "Epoch 0 | Step 15250 | Loss 1.2094\n",
      "Epoch 0 | Step 15500 | Loss 1.3550\n",
      "Training done...\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Training loop\n",
    "# ------------------------------------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    assert model.training and projector.training\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc='Trainig'):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Ship to cuda\n",
    "        input_ids = batch.input_ids.to(DEVICE)\n",
    "        attention_mask = batch.attention_mask.to(DEVICE)\n",
    "        labels = batch.labels.to(DEVICE)\n",
    "        obj_positions = batch.obj_positions.to(DEVICE)\n",
    "        pointclouds = batch.pointclouds.to(DEVICE).to(DTYPE)\n",
    "\n",
    "        # Get the token embeddings\n",
    "        base_embeds = model.get_input_embeddings()(input_ids)\n",
    "\n",
    "        # Get the point cloud feature \n",
    "        with torch.no_grad():\n",
    "            shape_feats = point_encoder(pointclouds).squeeze(1)\n",
    "\n",
    "        # Project the point cloud feature \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=DTYPE):\n",
    "            proj_vec = projector(shape_feats)\n",
    "\n",
    "        # Replace the point cloud feature for each example with the POINT_CLOUD_TOKEN placeholder embedding\n",
    "        for b in range(base_embeds.size(0)):\n",
    "            pos = obj_positions[b].item()\n",
    "            \n",
    "            if 0 <= pos < base_embeds.size(1):\n",
    "                # TODO write your code here.\n",
    "                base_embeds[b, pos, :] = proj_vec[b]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"there is no <POINT_CLOUD_TOKEN> token in the input!\")\n",
    "\n",
    "        # Predict the output\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=DTYPE):\n",
    "            out = model(inputs_embeds=base_embeds.cuda(), attention_mask=attention_mask.cuda(), labels=labels.cuda())\n",
    "            loss = out.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % 250 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {str(step).zfill(4)} | Loss {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c06da419-a421-4ec3-b231-f109a323d6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projector weights saved to projector_weights_499998.pth\n",
      "Merged model saved to my_full_finetuned_model\n",
      "Tokenizer saved to my_full_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Saving the model and the projector \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Save the projector weights (done for you)\n",
    "projector_save_path = f\"projector_weights_{len(train_ds)}.pth\"\n",
    "torch.save(projector.state_dict(), projector_save_path)\n",
    "print(f\"Projector weights saved to {projector_save_path}\")    \n",
    "\n",
    "# Save the model weights\n",
    "# Merge LoRA adapters into the base model, use merge_and_unload and save_pretrained functions\n",
    "# Do we need to save also the tokenizer?\n",
    "# Yes, I need to save the tokenizer because it contains the special tokens that i added, \n",
    "# and inference requires the same tokenizer to properly encode prompts and decode outputs.\n",
    "merged_model_save_dir = \"my_full_finetuned_model\"\n",
    "\n",
    "assert isinstance(model, PeftModel)\n",
    "\n",
    "# TODO write your code here\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_model_save_dir)\n",
    "tokenizer.save_pretrained(merged_model_save_dir)\n",
    "\n",
    "print(f\"Merged model saved to {merged_model_save_dir}\")\n",
    "print(f\"Tokenizer saved to {merged_model_save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6ce11eca-4113-454b-9177-e78d5ed6217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'my_full_finetuned_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and projector loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1005666/1692920574.py:9: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# load saved checkpoints\n",
    "# ------------------------------------------------------------\n",
    "# TODO write your code here. For the model use AutoModelForCausalLM.from_pretrained, what about the tokenizer?\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(merged_model_save_dir)\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(merged_model_save_dir)\n",
    "loaded_projector = Projector(THREE_D_DIM, llm_hidden_dim)\n",
    "loaded_projector.load_state_dict(torch.load(projector_save_path))\n",
    "loaded_projector.to(DEVICE).to(DTYPE)\n",
    "loaded_projector.eval()\n",
    "\n",
    "print(\"Model, tokenizer, and projector loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f6cad3e9-24bb-4dd8-9aff-e933b504f13b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://23e2bc8b08f7994019.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://23e2bc8b08f7994019.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: visualize a single point cloud using Plotly\n",
    "# ------------------------------------------------------------\n",
    "def visualize_pointcloud(points):\n",
    "    \"\"\"\n",
    "    Visualize a 3D colored point cloud (Nx6 tensor or array: [x,y,z,r,g,b])\n",
    "    \"\"\"\n",
    "    xyz = points[:, :3]\n",
    "    rgb = points[:, 3:]\n",
    "\n",
    "    # Normalize color if it's in [0,255]\n",
    "    if rgb.max() > 1.0:\n",
    "        rgb = rgb / 255.0\n",
    "\n",
    "    # Convert to 0â€“255 range for Plotly\n",
    "    colors = (rgb * 255).astype(int)\n",
    "    color_strings = [\n",
    "        f\"rgb({r},{g},{b})\" for r, g, b in colors\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[go.Scatter3d(\n",
    "            x=xyz[:, 0],\n",
    "            y=xyz[:, 1],\n",
    "            z=xyz[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                opacity=0.9,\n",
    "                color=color_strings\n",
    "            )\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z',\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Caption generation wrapper for Gradio\n",
    "# ------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def generate_caption(base, shape_feats, input_ids, obj_positions, method, max_new_tokens=128) -> str:\n",
    "    obj_pos = obj_positions[0]\n",
    "    proj = projector(shape_feats)\n",
    "    base[0, obj_pos, :] = proj[0]\n",
    "    attention_mask=torch.ones(base.shape[:-1], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    if method == 'greedy':\n",
    "        # TODO write your code here\n",
    "        gen_ids = model.generate(\n",
    "           inputs_embeds=base,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False\n",
    "        )\n",
    "    elif method == 'top-p':\n",
    "        # TODO write your code here\n",
    "        # use adequate parameters\n",
    "        gen_ids = model.generate(\n",
    "           inputs_embeds=base,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid generation method\")\n",
    "\n",
    "    # return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    if isinstance(gen_ids, list):\n",
    "        gen_ids = torch.tensor(gen_ids)\n",
    "    return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Function to run inference for one batch from test_loader\n",
    "# ------------------------------------------------------------\n",
    "def run_inference(sample_idx=0, method='greedy', not_gradio=False):\n",
    "    model.eval()\n",
    "    projector.eval()\n",
    "\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        if step == sample_idx:\n",
    "            input_ids = batch.input_ids.to(DEVICE)\n",
    "            attention_mask = batch.attention_mask.to(DEVICE)\n",
    "            obj_positions = batch.obj_positions.to(DEVICE)\n",
    "            pointclouds = batch.pointclouds.to(DEVICE).to(DTYPE)\n",
    "            gt_caption = batch.gt_captions\n",
    "\n",
    "            with torch.no_grad():\n",
    "                base_embeds = model.get_input_embeddings()(input_ids)\n",
    "                shape_feats = point_encoder(pointclouds).to(DEVICE).to(DTYPE)\n",
    "                gen_caption = generate_caption(base_embeds, shape_feats, input_ids, obj_positions, method=method)\n",
    "\n",
    "            if not_gradio:\n",
    "                return gen_caption, gt_caption\n",
    "            else:\n",
    "                pc_fig = visualize_pointcloud(pointclouds[0].detach().cpu().float().numpy())\n",
    "                \n",
    "            return pc_fig, gen_caption, gt_caption\n",
    "    return None, \"No sample found\", \"\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Gradio UI\n",
    "# ------------------------------------------------------------\n",
    "demo = gr.Interface(\n",
    "    fn=run_inference,\n",
    "    inputs=[\n",
    "        gr.Slider(0, 500, value=0, step=1, label=\"Sample index\"), # Showing only the first 500 shapes \n",
    "        gr.Radio([\"greedy\", \"top-p\"], value=\"greedy\", label=\"Generation Method\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Plot(label=\"Point Cloud\"),\n",
    "        gr.Textbox(label=\"Generated Caption\"),\n",
    "        gr.Textbox(label=\"Ground Truth Caption\")\n",
    "    ],\n",
    "    title=\"3D Point Cloud Captioning Demo\",\n",
    "    description=\"Visualize a 3D point cloud from the test loader and generate captions using a pretrained model.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True, inline=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f47ad444-31eb-4f63-aea9-87115458a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://ebe2a19c5167b6261e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ebe2a19c5167b6261e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=run_inference,\n",
    "    inputs=[\n",
    "        gr.Slider(0, 500, value=0, step=1, label=\"Sample index\"), # Showing only the first 500 shapes \n",
    "        gr.Radio([\"greedy\", \"top-p\"], value=\"greedy\", label=\"Generation Method\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Plot(label=\"Point Cloud\"),\n",
    "        gr.Textbox(label=\"Generated Caption\"),\n",
    "        gr.Textbox(label=\"Ground Truth Caption\")\n",
    "    ],\n",
    "    title=\"3D Point Cloud Captioning Demo\",\n",
    "    description=\"Visualize a 3D point cloud from the test loader and generate captions using a pretrained model.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True, inline=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c54193da-901a-43d5-8d0d-5b4ae1184e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://2448e1edd5c264aa77.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2448e1edd5c264aa77.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=run_inference,\n",
    "    inputs=[\n",
    "        gr.Slider(0, 500, value=0, step=1, label=\"Sample index\"), # Showing only the first 500 shapes \n",
    "        gr.Radio([\"greedy\", \"top-p\"], value=\"greedy\", label=\"Generation Method\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Plot(label=\"Point Cloud\"),\n",
    "        gr.Textbox(label=\"Generated Caption\"),\n",
    "        gr.Textbox(label=\"Ground Truth Caption\")\n",
    "    ],\n",
    "    title=\"3D Point Cloud Captioning Demo\",\n",
    "    description=\"Visualize a 3D point cloud from the test loader and generate captions using a pretrained model.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True, inline=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "061ec927-41ab-4df3-b009-45415076880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* Running on public URL: https://9843af59408faacedf.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9843af59408faacedf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=run_inference,\n",
    "    inputs=[\n",
    "        gr.Slider(0, 500, value=0, step=1, label=\"Sample index\"), # Showing only the first 500 shapes \n",
    "        gr.Radio([\"greedy\", \"top-p\"], value=\"greedy\", label=\"Generation Method\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Plot(label=\"Point Cloud\"),\n",
    "        gr.Textbox(label=\"Generated Caption\"),\n",
    "        gr.Textbox(label=\"Ground Truth Caption\")\n",
    "    ],\n",
    "    title=\"3D Point Cloud Captioning Demo\",\n",
    "    description=\"Visualize a 3D point cloud from the test loader and generate captions using a pretrained model.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True, inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b1615e08-205f-45c8-bd1a-be8f234d1db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>GT_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are a few options:\\n\\n1. \"The trusty old ...</td>\n",
       "      <td>[An electrical weighing machine with a white c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here are three different captions for the shap...</td>\n",
       "      <td>[College walls]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here are three different captions for the shap...</td>\n",
       "      <td>[Grey coloured bat toy.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here's a possible caption for the input shape ...</td>\n",
       "      <td>[White thin panel with red edge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Red sports car, sleek and powerful, with a cu...</td>\n",
       "      <td>[Carro rojo y negro con cuatro llantas negras]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>\"Police Car: 2018 Ford F-150 Police Intercepto...</td>\n",
       "      <td>[A police car with a siren on the to p and hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Here's a possible caption for the 3D shape: \\n...</td>\n",
       "      <td>[A white building structure with two parts and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Here are three different captions for the shap...</td>\n",
       "      <td>[A cartoon girl with red eyes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Here's a caption for the shape \"attract\":\\n\\n\"...</td>\n",
       "      <td>[Red coloured toy dinosaur model.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>\"Golden spiral of 3D, a spiral shape formed by...</td>\n",
       "      <td>[A 3d model of a yellow color spring type of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    generated_caption                                         GT_caption\n",
       "0   Here are a few options:\\n\\n1. \"The trusty old ...  [An electrical weighing machine with a white c...\n",
       "1   Here are three different captions for the shap...                                    [College walls]\n",
       "2   Here are three different captions for the shap...                           [Grey coloured bat toy.]\n",
       "3   Here's a possible caption for the input shape ...                   [White thin panel with red edge]\n",
       "4   \"Red sports car, sleek and powerful, with a cu...     [Carro rojo y negro con cuatro llantas negras]\n",
       "..                                                ...                                                ...\n",
       "95  \"Police Car: 2018 Ford F-150 Police Intercepto...  [A police car with a siren on the to p and hav...\n",
       "96  Here's a possible caption for the 3D shape: \\n...  [A white building structure with two parts and...\n",
       "97  Here are three different captions for the shap...                     [A cartoon girl with red eyes]\n",
       "98  Here's a caption for the shape \"attract\":\\n\\n\"...                 [Red coloured toy dinosaur model.]\n",
       "99  \"Golden spiral of 3D, a spiral shape formed by...  [A 3d model of a yellow color spring type of t...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# saving greedy predictions, please make sure the csv files are saved and the cell output is shown\n",
    "tuples = []\n",
    "for i in range(100):\n",
    "    tuples.append(run_inference(sample_idx=i, method='greedy', not_gradio=True))\n",
    "\n",
    "greedy_preds = pd.DataFrame(tuples)\n",
    "greedy_preds.columns = [\"generated_caption\", \"GT_caption\"]\n",
    "greedy_preds.to_csv(\"greedy_preds.csv\")\n",
    "greedy_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "654adb5b-1cef-4386-932b-795ff5702478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>GT_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 3D model of a white lamp with a cord and plu...</td>\n",
       "      <td>[An electrical weighing machine with a white c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 3D model of a torn-apart house with graffiti...</td>\n",
       "      <td>[College walls]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 3D model featuring a bird, frog, lizard, and...</td>\n",
       "      <td>[Grey coloured bat toy.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A 3D rendering of a wooden door with a white f...</td>\n",
       "      <td>[White thin panel with red edge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A 3D model of a red sports car.</td>\n",
       "      <td>[Carro rojo y negro con cuatro llantas negras]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3D model of a police car with a white roof and...</td>\n",
       "      <td>[A police car with a siren on the to p and hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A 3D model of a building featuring yellow and ...</td>\n",
       "      <td>[A white building structure with two parts and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3D model of a woman in a yellow dress flying w...</td>\n",
       "      <td>[A cartoon girl with red eyes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A 3D model of a horse with a long tail and horns.</td>\n",
       "      <td>[Red coloured toy dinosaur model.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>A 3D model of a yellow spiral object resemblin...</td>\n",
       "      <td>[A 3d model of a yellow color spring type of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    generated_caption                                         GT_caption\n",
       "0   A 3D model of a white lamp with a cord and plu...  [An electrical weighing machine with a white c...\n",
       "1   A 3D model of a torn-apart house with graffiti...                                    [College walls]\n",
       "2   A 3D model featuring a bird, frog, lizard, and...                           [Grey coloured bat toy.]\n",
       "3   A 3D rendering of a wooden door with a white f...                   [White thin panel with red edge]\n",
       "4                     A 3D model of a red sports car.     [Carro rojo y negro con cuatro llantas negras]\n",
       "..                                                ...                                                ...\n",
       "95  3D model of a police car with a white roof and...  [A police car with a siren on the to p and hav...\n",
       "96  A 3D model of a building featuring yellow and ...  [A white building structure with two parts and...\n",
       "97  3D model of a woman in a yellow dress flying w...                     [A cartoon girl with red eyes]\n",
       "98  A 3D model of a horse with a long tail and horns.                 [Red coloured toy dinosaur model.]\n",
       "99  A 3D model of a yellow spiral object resemblin...  [A 3d model of a yellow color spring type of t...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving top-p predictions\n",
    "tuples = []\n",
    "for i in range(100):\n",
    "    tuples.append(run_inference(sample_idx=i, method='top-p', not_gradio=True))\n",
    "\n",
    "# saving top-p predictions, please make sure the csv files are saved and the cell output is shown\n",
    "top_p_preds = pd.DataFrame(tuples)\n",
    "top_p_preds.columns = [\"generated_caption\", \"GT_caption\"]\n",
    "top_p_preds.to_csv(\"top_p_preds.csv\")\n",
    "top_p_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "25264962-b8e2-40ec-b044-9fb4274bf0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    generated_caption                                         GT_caption\n",
      "0   Here are a few options:\\n\\n1. \"The trusty old ...  [An electrical weighing machine with a white c...\n",
      "1   Here are three different captions for the shap...                                    [College walls]\n",
      "2   Here are three different captions for the shap...                           [Grey coloured bat toy.]\n",
      "3   Here's a possible caption for the input shape ...                   [White thin panel with red edge]\n",
      "4   \"Red sports car, sleek and powerful, with a cu...     [Carro rojo y negro con cuatro llantas negras]\n",
      "..                                                ...                                                ...\n",
      "95  \"Police Car: 2018 Ford F-150 Police Intercepto...  [A police car with a siren on the to p and hav...\n",
      "96  Here's a possible caption for the 3D shape: \\n...  [A white building structure with two parts and...\n",
      "97  Here are three different captions for the shap...                     [A cartoon girl with red eyes]\n",
      "98  Here's a caption for the shape \"attract\":\\n\\n\"...                 [Red coloured toy dinosaur model.]\n",
      "99  \"Golden spiral of 3D, a spiral shape formed by...  [A 3d model of a yellow color spring type of t...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(greedy_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3356a4bf-dc30-4c18-b0df-5088b996fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    generated_caption                                         GT_caption\n",
      "0   A 3D model of a white lamp with a cord and plu...  [An electrical weighing machine with a white c...\n",
      "1   A 3D model of a torn-apart house with graffiti...                                    [College walls]\n",
      "2   A 3D model featuring a bird, frog, lizard, and...                           [Grey coloured bat toy.]\n",
      "3   A 3D rendering of a wooden door with a white f...                   [White thin panel with red edge]\n",
      "4                     A 3D model of a red sports car.     [Carro rojo y negro con cuatro llantas negras]\n",
      "..                                                ...                                                ...\n",
      "95  3D model of a police car with a white roof and...  [A police car with a siren on the to p and hav...\n",
      "96  A 3D model of a building featuring yellow and ...  [A white building structure with two parts and...\n",
      "97  3D model of a woman in a yellow dress flying w...                     [A cartoon girl with red eyes]\n",
      "98  A 3D model of a horse with a long tail and horns.                 [Red coloured toy dinosaur model.]\n",
      "99  A 3D model of a yellow spiral object resemblin...  [A 3d model of a yellow color spring type of t...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(top_p_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404144d-7b3a-4729-9c78-3d7e34f0e14a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
